{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hulk Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import errors\n",
    "from Lexer.Lexer_generator import Lexer\n",
    "from Parser.Parser import ParserError, LR1Parser\n",
    "from Grammar.Grammar import G\n",
    "from Lexer.Tokens_lexer import tokens\n",
    "from Semantic.semantic_analysis_pipeline import semantic_analysis_pipeline\n",
    "from Code_gen.code_generator import CCodeGenerator\n",
    "from cmp.evaluation import evaluate_reverse_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_error(message):\n",
    "    red = \"\\033[31m\"\n",
    "    refresh = \"\\033[0m\"\n",
    "    print(f\"{red}{message}{refresh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexer= Lexer(tokens,G.EOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=LR1Parser(G) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(input_path: Path, output_path: Path):\n",
    "    if not input_path.match('*.hulk'):\n",
    "        raise errors.HulkIOError(errors.HulkIOError.INVALID_EXTENSION % input_path)\n",
    "\n",
    "    try:\n",
    "        with open(input_path) as f:\n",
    "            text = f.read()\n",
    "    except FileNotFoundError:\n",
    "        error = errors.HulkIOError(errors.HulkIOError.ERROR_READING_FILE % input_path)\n",
    "        print_error(error)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        tokens_=lexer.tokenize(text)\n",
    "    except errors.LexerError as e:\n",
    "        print_error(e)\n",
    "        return\n",
    "    \n",
    "\n",
    "    try: \n",
    "        parse, operations = parser([t.token_type for t in tokens_])\n",
    "    except ParserError as e:\n",
    "        error_token = tokens_[e.token_index]\n",
    "        error_text = errors.HulkSyntacticError.PARSING_ERROR % error_token.lex\n",
    "        error_ = [errors.HulkSyntacticError(error_text, error_token.row, error_token.column,)]\n",
    "        print_error(error_)\n",
    "        return\n",
    "    \n",
    "    ast=evaluate_reverse_parse(parse,operations,tokens_)\n",
    "\n",
    "    semantic_ast, semantic_errors,context,_=semantic_analysis_pipeline(ast)\n",
    "\n",
    "    if semantic_errors:\n",
    "        for e in semantic_errors:\n",
    "            print_error(e)\n",
    "        return\n",
    "    \n",
    "    codegen=CCodeGenerator()\n",
    "    code= codegen(semantic_ast,context)\n",
    "\n",
    "    try:\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(code)\n",
    "    except FileNotFoundError:\n",
    "        error = errors.HulkIOError(errors.HulkIOError.ERROR_WRITING_FILE % output_path)\n",
    "        print_error(error)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    subprocess.run([\"gcc\",\"-o\",'archivo','archivo.c', \"-lm\"])\n",
    "    subprocess.run(['./archivo'])\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline(Path('archivo.hulk'), Path('archivo.c'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
